{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_files(path,directory):\n",
    "    file_names = []\n",
    "    ratings = []\n",
    "    labels = []\n",
    "    file_list = os.listdir(os.path.join(path,directory))\n",
    "    for f in file_list:\n",
    "        if f[-4:] == '.txt':\n",
    "            file_names.append(os.path.join(path,directory) + '/'+f)\n",
    "            ratings.append(int(os.path.splitext(f)[0].split('_')[1]))\n",
    "    if directory == 'pos':\n",
    "        labels = [1]*len(file_names)\n",
    "    else:\n",
    "        labels = [0]*len(file_names)\n",
    "    \n",
    "    return file_names, ratings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_pos_files, train_pos_ratings, train_pos_labels = process_files('./../aclImdb/train/','pos')\n",
    "# train_neg_files, train_neg_ratings, train_neg_labels = process_files('./../aclImdb/train/','neg')\n",
    "# test_pos_files, test_pos_ratings, test_pos_labels = process_files('./../aclImdb/test/','pos')\n",
    "# test_neg_files, test_neg_ratings, test_neg_labels = process_files('./../aclImdb/test/','neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_files,train_ratings, train_labels = np.array(train_pos_files + train_neg_files),np.array(train_pos_ratings + train_neg_ratings), np.array(train_pos_labels + train_neg_labels)\n",
    "# test_files,test_ratings, test_labels = np.array(test_pos_files + test_neg_files),np.array(test_pos_ratings + test_neg_ratings), np.array(test_pos_labels + test_neg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_idx, val_idx = train_test_split(np.arange(len(train_files)),test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val_files,val_ratings,val_labels = train_files[val_idx],train_ratings[val_idx],train_labels[val_idx]\n",
    "# train_files_1,train_ratings_1,train_labels_1 = train_files[train_idx],train_ratings[train_idx],train_labels[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickling(file,path):\n",
    "    pickle.dump(file,open(path,'wb'))\n",
    "def unpickling(path):\n",
    "    file_return=pickle.load(open(path,'rb'))\n",
    "    return file_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling([train_files_1,train_ratings_1, train_labels_1],'req_train_files.p')\n",
    "[train_files_1,train_ratings_1, train_labels_1] = unpickling('req_train_files.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling([test_files,test_ratings, test_labels],'req_test_files.p')\n",
    "[test_files,test_ratings, test_labels] = unpickling('req_test_files.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling([val_files,val_ratings, val_labels],'req_val_files.p')\n",
    "[val_files,val_ratings, val_labels] = unpickling('req_val_files.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'is', \"n't\", 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "# lem = nltk.stem.LancasterStemmer()\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "# nltk.download(\"wordnet\")\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "#     temp_list = []\n",
    "#     for token in tokens:\n",
    "#         if (token.text.lower() not in stop_words):\n",
    "#             if (token.text.lower() not in punctuations):\n",
    "#                 temp_list.append(token.text.lower())\n",
    "#     return temp_list\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     return [token.text for token in tokens]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u\"Apples isn't looking at buying U.K. startup for $1 billion\")\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_n(array,n):\n",
    "    grams = ngrams(array,n)\n",
    "    return [' '.join(g) for g in grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_till_n(sent,n = 1):\n",
    "    tokens = tokenize(sent)\n",
    "    final_list = [];\n",
    "    if n == 1:\n",
    "        final_list = tokens\n",
    "    else:\n",
    "        for i in range(1, n+1):\n",
    "            final_list += tokenize_n(tokens, i)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset,n):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for path in dataset:\n",
    "        sample = open(path,'r',encoding=\"utf8\").read()\n",
    "        tokens = tokenize_till_n(sample, n = n)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_no_preprocessing_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,1)\n",
    "# pickling(test_data_tokens, \"test_data_tokens_n=1.p\")\n",
    "# pickling(test_data_tokens, \"test_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,1)\n",
    "# pickling(train_data_tokens,\"train_data_tokens_n=1.p\")\n",
    "# pickling(train_data_tokens,\"train_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(all_tokens, \"all_train_tokens_n=1.p\")\n",
    "# pickling(all_tokens, \"all_train_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_n=2.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,2)\n",
    "# pickling(test_data_tokens, \"test_data_tokens_n=2.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,2)\n",
    "# pickling(train_data_tokens,\"train_data_tokens_n=2.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(all_tokens, \"all_train_tokens_n=2.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_n=3.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,3)\n",
    "# pickling(test_data_tokens, \"test_data_tokens_n=3.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,3)\n",
    "# pickling(train_data_tokens,\"train_data_tokens_n=3.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(all_tokens, \"all_train_tokens_n=3.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_n=4.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,4)\n",
    "# pickling(test_data_tokens, \"test_data_tokens_n=4.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,4)\n",
    "# pickling(train_data_tokens,\"train_data_tokens_n=4.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(all_tokens, \"all_train_tokens_n=4.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 1 (no preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "#     temp_list = []\n",
    "#     for token in tokens:\n",
    "#         if (token.text.lower() not in stop_words):\n",
    "#             if (token.text.lower() not in punctuations):\n",
    "#                 temp_list.append(token.text.lower())\n",
    "#     return temp_list\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_no_preprocessing_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,1)\n",
    "# pickling(test_data_tokens, \"test_data_tokens_n=1.p\")\n",
    "# pickling(test_data_tokens, \"test_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,1)\n",
    "# pickling(train_data_tokens,\"train_data_tokens_n=1.p\")\n",
    "# pickling(train_data_tokens,\"train_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(all_tokens, \"all_train_tokens_n=1.p\")\n",
    "# pickling(all_tokens, \"all_train_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens without stop words (n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"to\",\"from\",\"up\",\"down\",\"in\",\"on\",\"off\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"other\",\"some\",\"such\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    temp_list = []\n",
    "    for token in tokens:\n",
    "        if (token.text.lower() not in stop_words):\n",
    "            if (token.text.lower() not in punctuations):\n",
    "                temp_list.append(token.text.lower())\n",
    "    return temp_list\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(val_data_tokens,'val_data_tokens_wo_stop_words_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_no_preprocessing_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,1)\n",
    "pickling(test_data_tokens, \"test_data_wo_stop_tokens_n=1.p\")\n",
    "# pickling(test_data_tokens, \"test_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,1)\n",
    "pickling(train_data_tokens,\"train_data_wo_stop_tokens_n=1.p\")\n",
    "# pickling(train_data_tokens,\"train_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(all_tokens, \"all_train_wo_stop_tokens_n=1.p\")\n",
    "# pickling(all_tokens, \"all_train_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 2 (wo stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(val_data_tokens,'val_data_tokens_wo_stop_words_n=2.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_no_preprocessing_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,2)\n",
    "pickling(test_data_tokens, \"test_data_wo_stop_tokens_n=2.p\")\n",
    "# pickling(test_data_tokens, \"test_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,2)\n",
    "pickling(train_data_tokens,\"train_data_wo_stop_tokens_n=2.p\")\n",
    "# pickling(train_data_tokens,\"train_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(all_tokens, \"all_train_wo_stop_tokens_n=2.p\")\n",
    "# pickling(all_tokens, \"all_train_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 3 (wo stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(val_data_tokens,'val_data_tokens_wo_stop_words_n=3.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_no_preprocessing_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,3)\n",
    "pickling(test_data_tokens, \"test_data_wo_stop_tokens_n=3.p\")\n",
    "# pickling(test_data_tokens, \"test_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,3)\n",
    "pickling(train_data_tokens,\"train_data_wo_stop_tokens_n=3.p\")\n",
    "# pickling(train_data_tokens,\"train_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(all_tokens, \"all_train_wo_stop_tokens_n=3.p\")\n",
    "# pickling(all_tokens, \"all_train_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N = 4 (wo stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_files,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(val_data_tokens,'val_data_tokens_wo_stop_words_n=4.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickling(val_data_tokens,'val_data_tokens_no_preprocessing_n=1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_files,4)\n",
    "pickling(test_data_tokens, \"test_data_wo_stop_tokens_n=4.p\")\n",
    "# pickling(test_data_tokens, \"test_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_tokens = tokenize_dataset(train_files_1,4)\n",
    "pickling(train_data_tokens,\"train_data_wo_stop_tokens_n=4.p\")\n",
    "# pickling(train_data_tokens,\"train_data_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickling(all_tokens, \"all_train_wo_stop_tokens_n=4.p\")\n",
    "# pickling(all_tokens, \"all_train_tokens_no_preprocessing_n=1.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
